{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class hh:\n",
    "    def parse_config_file(filepath):\n",
    "        # Regex patterns to identify the different line types\n",
    "        header_pattern = r\"header\\s+(\\d+\\.\\d+\\.\\d+\\.\\d+\\.\\d+)\"\n",
    "        # Updated value pattern to robustly handle all numerical formats\n",
    "        value_pattern = r\"^(?!==)([^\\t]+?)\\t+.*?([\\-\\d\\.]+(?:[Ee][+-]?\\d+)?)(\\s*[\\w\\/]*)$\"\n",
    "        \n",
    "        # Read the file and split into paragraphs using 'header' as the starting point of a new paragraph\n",
    "        with open(filepath, 'r') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # DataFrame to hold all the data\n",
    "        df = pd.DataFrame()\n",
    "        paragraphs = re.split(r'\\n(?=header)', content)\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            data_dict = {}\n",
    "            lines = paragraph.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                header_match = re.match(header_pattern, line)\n",
    "                value_match = re.match(value_pattern, line)\n",
    "                \n",
    "                if header_match:\n",
    "                    run, sequence, block, trace, _ = header_match.group(1).split('.')\n",
    "                    data_dict.update({'run': run, 'sequence': sequence, 'block': block, 'trace': trace})\n",
    "                elif line.startswith(\"Start Time\") or line.startswith(\"End Time\"):\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 1:\n",
    "                        timestamp = parts[1].strip()\n",
    "                        try:\n",
    "                            dt = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "                            prefix = \"start\" if \"Start Time\" in line else \"end\"\n",
    "                            data_dict[f\"{prefix} year\"] = dt.year\n",
    "                            data_dict[f\"{prefix} month\"] = dt.month\n",
    "                            data_dict[f\"{prefix} day\"] = dt.day\n",
    "                            data_dict[f\"{prefix} hour\"] = dt.hour\n",
    "                            data_dict[f\"{prefix} minute\"] = dt.minute\n",
    "                            data_dict[f\"{prefix} second\"] = dt.second + dt.microsecond / 1e6\n",
    "                        except ValueError:\n",
    "                            pass  # Ignore invalid datetime values\n",
    "                elif value_match:\n",
    "                    key = value_match.group(1).strip()\n",
    "                    value = value_match.group(2).strip()\n",
    "                    try:\n",
    "                        float_val = float(value)\n",
    "                        if float_val.is_integer():\n",
    "                            value = int(float_val)\n",
    "                        else:\n",
    "                            value = float_val\n",
    "                    except ValueError:\n",
    "                        continue  # If conversion fails, skip adding this entry\n",
    "                    if key not in data_dict:\n",
    "                        data_dict[key] = value\n",
    "                    else:\n",
    "                        pass\n",
    "            \n",
    "            # Convert the dictionary to a DataFrame row and append it to the main DataFrame\n",
    "            row_df = pd.DataFrame([data_dict])\n",
    "            df = pd.concat([df, row_df], ignore_index=True)\n",
    "\n",
    "        # Ensure all missing values are filled with NaN and reorder columns placing identifiers first\n",
    "        identifier_cols = ['run', 'sequence', 'block', 'trace']\n",
    "        other_cols = [col for col in df.columns if col not in identifier_cols]\n",
    "        final_cols = identifier_cols + other_cols\n",
    "        df = df.reindex(columns=final_cols)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def headerHandler(list_of_target_folders):\n",
    "        all_dfs = []  # This list will store all the DataFrames to be concatenated.\n",
    "        \n",
    "        # Iterate over each directory in the list\n",
    "        for folder in list_of_target_folders:\n",
    "            # Use glob to find all .txt files in the current folder\n",
    "            txt_files = glob.glob(os.path.join(folder, '*.txt'))\n",
    "            \n",
    "            # Process each file found\n",
    "            for file_path in txt_files:\n",
    "                try:\n",
    "                    # Parse the configuration file to a DataFrame\n",
    "                    df = hh.parse_config_file(file_path)\n",
    "                    all_dfs.append(df)  # Add the resulting DataFrame to the list\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {file_path}: {str(e)}\")\n",
    "        \n",
    "        # Concatenate all DataFrames into one, if any are found\n",
    "        if all_dfs:\n",
    "            final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        else:\n",
    "            final_df = pd.DataFrame()  # Return an empty DataFrame if no files were processed\n",
    "                \n",
    "        final_df['run'] = final_df['run'].astype(int)\n",
    "        final_df['sequence'] = final_df['sequence'].astype(int)\n",
    "        final_df['block'] = final_df['block'].astype(int)\n",
    "        final_df['trace'] = final_df['trace'].astype(int)\n",
    "\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_target_folders = [r\"C:\\ACME_analysis\\doppler585\"]\n",
    "\n",
    "xx=hh.headerHandler(list_of_target_folders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
