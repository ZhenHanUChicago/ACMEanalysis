{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edmAnalyzer import parityStateTransform, combine_switches\n",
    "from edmAnalyzer import hh\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from cryptography.hazmat.primitives import serialization, hashes\n",
    "from cryptography.hazmat.primitives.asymmetric import padding\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "class sequenceCalculator:\n",
    "\n",
    "    class blind_object:\n",
    "        def __init__(self, blind_value_in_rad_s, blind_id):\n",
    "            self._blind_value_in_rad_s = blind_value_in_rad_s\n",
    "            self.blind_id = blind_id\n",
    "\n",
    "        def __str__(self):\n",
    "            return f\"************rad/s, blind name: {self.blind_id}\"\n",
    "\n",
    "        def __repr__(self):\n",
    "            return f\"************rad/s, blind name: {self.blind_id}\"\n",
    "        \n",
    "    class Parameters:\n",
    "        def __init__(self):\n",
    "            self.non_parity_switches =  []\n",
    "            self.superblock_parity_switches = [\"P\", \"L\", \"R\"]\n",
    "\n",
    "        def _load_parameters_from_json(self, parameter_file_path):\n",
    "            try:\n",
    "                with open(parameter_file_path, 'r') as f:\n",
    "                    param_dict = json.load(f)\n",
    "                \n",
    "                for key, value in param_dict.items():\n",
    "                    if hasattr(self, key) and value is not None:\n",
    "                        setattr(self, key, value)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Sequence switch specification {parameter_file_path} not found, using default values.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Sequence: Error decoding JSON file {parameter_file_path}, using default values.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Sequence: Unexpected error: {e}\")\n",
    "\n",
    "    class SequenceResults:\n",
    "\n",
    "        class Blinded:\n",
    "            def __init__(self):\n",
    "                self.blind_id = None\n",
    "                self.result = {}\n",
    "                self.result_summary = {}\n",
    "                self.superblock_state_result = {}\n",
    "                self.final_result = {}\n",
    "\n",
    "        class Unblinded:\n",
    "            def __init__(self):\n",
    "                self.result = {}\n",
    "                self.result_summary = {}\n",
    "                self.superblock_state_result = {}\n",
    "                self.final_result = {}\n",
    "\n",
    "        def __init__(self):\n",
    "            self.blinded = self.Blinded()\n",
    "            self.__unblinded = self.Unblinded()\n",
    "            self.sequence_range = None\n",
    "            self.sequence_run_sequence = None\n",
    "            self.sequence_type = None\n",
    "            self.non_parity_switches = []\n",
    "            self.superblock_parity_switches = []\n",
    "            self.superblock_parity_labels = None\n",
    "            self.superblock_state_labels = None\n",
    "            self.sequence_name = None\n",
    "            self.sequence_string = None\n",
    "            self.labels = None\n",
    "            self.blockdf = None\n",
    "            self.sequencedf = None\n",
    "            self.sequencesipmdf = None\n",
    "            self.sequence_group_left = None\n",
    "            self.sequence_group_right = None\n",
    "\n",
    "    def read_blind(self, folder_path):\n",
    "        # Define the file paths\n",
    "        blind_bytes_path = os.path.join(folder_path, 'blind_bytes.txt')\n",
    "        private_key_path = os.path.join(folder_path, 'private_key.txt')\n",
    "\n",
    "        # Try to read the blind bytes from the txt file\n",
    "        try:\n",
    "            with open(blind_bytes_path, 'rb') as f:\n",
    "                extracted_bytes = f.read()\n",
    "        except FileNotFoundError as e:\n",
    "            raise Exception(f\"Failed to open blind bytes file: {blind_bytes_path}\") from e\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred while reading the blind bytes file: {blind_bytes_path}\") from e\n",
    "\n",
    "        # Try to read the private key from the file\n",
    "        try:\n",
    "            with open(private_key_path, 'rb') as f:\n",
    "                private_key_data = f.read()\n",
    "        except FileNotFoundError as e:\n",
    "            raise Exception(f\"Failed to open private key file: {private_key_path}\") from e\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred while reading the private key file: {private_key_path}\") from e\n",
    "\n",
    "        # Skip the first line of the private key and extract the last four characters of the second line\n",
    "        try:\n",
    "            private_key_lines = private_key_data.splitlines()\n",
    "            key_id_start = private_key_lines[1][-4:].decode('utf-8', errors='ignore')\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Failed to extract the last four letters from the second line of the private key\") from e\n",
    "\n",
    "        # Try to deserialize the private key\n",
    "        try:\n",
    "            private_key = serialization.load_pem_private_key(\n",
    "                private_key_data,\n",
    "                password=None\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Failed to deserialize the private key\") from e\n",
    "\n",
    "        # Try to decrypt the blind using the private key\n",
    "        try:\n",
    "            decrypted_blind = private_key.decrypt(\n",
    "                extracted_bytes,\n",
    "                padding.OAEP(\n",
    "                    mgf=padding.MGF1(algorithm=hashes.SHA256()),\n",
    "                    algorithm=hashes.SHA256(),\n",
    "                    label=None\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Failed to decrypt the blind value\") from e\n",
    "\n",
    "        # Try to convert the decrypted blind back to a double\n",
    "        try:\n",
    "            blind_value_in_rad_s = np.frombuffer(decrypted_blind, dtype=np.float64)[0]\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Failed to convert decrypted blind to float64\") from e\n",
    "\n",
    "        # Create the blind ID using the folder name and the last four letters of the second line of the private key\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        blind_id = f\"{folder_name}-{key_id_start}\"\n",
    "\n",
    "        # Return the blind_object\n",
    "        self.blind =  sequenceCalculator.blind_object(blind_value_in_rad_s, blind_id)\n",
    "\n",
    "    def __init__(self, sequence_json_path = None, blockresults_path_list = None, blockcutresults_path_list = None, sequence_result_folder_path = None, df = None, blind_path = None, sequence_name = None):\n",
    "        self.sequence_result_folder_path = sequence_result_folder_path\n",
    "        self.blockresults_path_list = blockresults_path_list\n",
    "        self.blockcutresults_path_list = blockcutresults_path_list\n",
    "        \n",
    "        self.blind = None\n",
    "        self.read_blind(blind_path)\n",
    "        self.parameters = self.Parameters()\n",
    "        self.parameters._load_parameters_from_json(sequence_json_path)\n",
    "\n",
    "        self.P, self.superblock_parity_labels, self.superblock_state_labels = parityStateTransform(channelName= self.parameters.superblock_parity_switches)\n",
    "        _, self.block_parity_labels, self.block_state_labels = parityStateTransform(channelName= ['N','E','B'])\n",
    "        _, self.labels = combine_switches(self.superblock_parity_labels, self.block_parity_labels)\n",
    "\n",
    "        self.sequenceresult = self.SequenceResults()\n",
    "        self.sequenceresult.blinded.blind_id = self.blind.blind_id\n",
    "        self.sequenceresult.superblock_parity_labels = self.superblock_parity_labels\n",
    "        self.sequenceresult.superblock_state_labels = self.superblock_state_labels\n",
    "        self.sequenceresult.labels = self.labels\n",
    "                \n",
    "\n",
    "        self.sequenceresult.non_parity_switches = self.parameters.non_parity_switches\n",
    "        self.non_parity_switches = self.parameters.non_parity_switches\n",
    "\n",
    "        self.sequenceresult.superblock_parity_switches = self.parameters.superblock_parity_switches\n",
    "        self.superblock_parity_switches = self.parameters.superblock_parity_switches\n",
    "    \n",
    "        self.sequenceresult.sequence_type = os.path.splitext(os.path.basename(sequence_json_path))[0]\n",
    "\n",
    "        self.sequenceresult.sequence_range = []\n",
    "        for path in blockresults_path_list:\n",
    "            filename = os.path.basename(path)  # Get the file name from the path\n",
    "            parts = filename.replace('blockresult_', '').replace('.pkl', '').split('.')  # Split to get run, sequence, block\n",
    "            run = int(parts[0])  # Extract run number\n",
    "            sequence = int(parts[1])  # Extract sequence number\n",
    "            block = int(parts[2])  # Extract block number\n",
    "            self.sequenceresult.sequence_range.append((run, sequence, block))  # Append (run, sequence) to the list\n",
    "        \n",
    "        self.sequenceresult.sequence_range.sort()\n",
    "        self.sequence_run_seqeuence = sorted(list(set([(r[0], r[1]) for r in self.sequenceresult.sequence_range])))\n",
    "\n",
    "        sequence_string = \"\"\n",
    "        for run,sequence in self.sequence_run_seqeuence:\n",
    "            sequence_string += \"_\" + str(int(run)).zfill(4) + \".\" + str(int(sequence)).zfill(4)\n",
    "        \n",
    "        self.sequenceresult.sequence_string = sequence_string\n",
    "        self.sequenceresult.sequence_run_sequence = self.sequence_run_seqeuence\n",
    "        self.sequenceresult.sequence_name = sequence_name\n",
    "        self.df = df\n",
    "\n",
    "        self._load_blockresults()\n",
    "        self._create_superblock_state_dict()\n",
    "        self._create_sequence_results()\n",
    "        self._deep_copy_results()\n",
    "        self._apply_blinding()\n",
    "        self.saveresults()\n",
    "\n",
    "    def blockresult_to_df(b):\n",
    "        flat_dict = {}\n",
    "        \n",
    "        # Extract run, sequence, and block from b.block_string\n",
    "        block_string = b.block_string\n",
    "        run, sequence, block = [int(x) for x in block_string.split('.')]\n",
    "        flat_dict['run'] = run\n",
    "        flat_dict['sequence'] = sequence\n",
    "        flat_dict['block'] = block\n",
    "        for key, matrix in b.blinded.result_summary.items():\n",
    "            # Remove \"_summary\" from the key\n",
    "            base_name = key.replace('_summary', '')\n",
    "            \n",
    "            # Determine if it's a variance (starts with 'd' and ends with '2')\n",
    "            is_variance = key.startswith('d') and key.endswith('2_summary')\n",
    "            \n",
    "            if is_variance:\n",
    "                # Strip 'd' and '2' from base_name\n",
    "                base_name = base_name[1:-1]\n",
    "            \n",
    "            # Get the size of the matrix\n",
    "            shape = matrix.shape\n",
    "            \n",
    "            # Iterate over each element of the matrix\n",
    "            for i in range(shape[0]):  # Adjust dimension index based on shape\n",
    "                for p in range(shape[3]):  # 'p' is for SIPM in [k, 1, 1, p, 1]\n",
    "                    # Get the parity label from b.parity_labels\n",
    "                    channel = b.parity_labels[i]\n",
    "                    \n",
    "                    # Determine the SIPM number (sipm = p)\n",
    "                    sipm = p\n",
    "                    \n",
    "                    # Build the name for this entry\n",
    "                    if is_variance:\n",
    "                        name = f\"blockuncertainty_{base_name}_{channel}_sipm{sipm}\"\n",
    "                        value = np.sqrt(matrix[i, 0, 0, p, 0])  # Take sqrt for variance\n",
    "                    else:\n",
    "                        name = f\"block_{base_name}_{channel}_sipm{sipm}\"\n",
    "                        value = matrix[i, 0, 0, p, 0]  # Leave value as is for quantity\n",
    "                    \n",
    "                    # Add run, sequence, and block information\n",
    "                    flat_dict[name] = value\n",
    "\n",
    "        \n",
    "        # Convert the flat dictionary to a one-line DataFrame\n",
    "        df = pd.DataFrame([flat_dict])\n",
    "        return df\n",
    "\n",
    "    def _load_blockresults(self):\n",
    "        # Initialize the dictionary to store the block results\n",
    "        self.sequence_begin_group = 0\n",
    "        self.sequence_end_group = 10000\n",
    "\n",
    "        self.blockdict = {}\n",
    "        self.sequenceresult.blockdf = []\n",
    "        # Loop over all blockresult files in the blockresults_path_list\n",
    "        for blockresult_path in self.blockresults_path_list:\n",
    "            # Extract run, sequence, and block number from the filename\n",
    "            filename = os.path.basename(blockresult_path)\n",
    "            parts = filename.replace('blockresult_', '').replace('.pkl', '').split('.')\n",
    "            run, sequence, block = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "\n",
    "            # Construct the corresponding blockcutresult file path\n",
    "            blockcutresult_path = blockresult_path.replace('blockresult_', 'blockcutresult_')\n",
    "\n",
    "            # Load the blockresult and blockcutresult pkl files\n",
    "            with open(blockresult_path, 'rb') as f:\n",
    "                blockresult = pickle.load(f)\n",
    "\n",
    "            with open(blockcutresult_path, 'rb') as f:\n",
    "                blockcutresult = pickle.load(f)\n",
    "\n",
    "            # Check if the block stays based on blockcutresult\n",
    "            if not blockcutresult.does_this_block_stay:\n",
    "                continue  # Skip this block if it doesn't stay\n",
    "\n",
    "            self.sequence_begin_group = max(self.sequence_begin_group, blockresult.blockcut_left)\n",
    "            self.sequence_end_group = min(self.sequence_end_group, blockresult.blockcut_right)\n",
    "\n",
    "            block_df = sequenceCalculator.blockresult_to_df(blockresult)\n",
    "            \n",
    "\n",
    "\n",
    "            # Quantities from blockresult to be used as first layer keys\n",
    "            quantities = {\n",
    "                'C': blockresult._BlockResults__unblinded.result['C'],\n",
    "                'phi': blockresult._BlockResults__unblinded.result['phi'],\n",
    "                'omega': blockresult._BlockResults__unblinded.result['omega'],\n",
    "                'tau': blockresult._BlockResults__unblinded.result['tau'],\n",
    "                'dC2': blockresult._BlockResults__unblinded.result['dC2'],\n",
    "                'dphi2': blockresult._BlockResults__unblinded.result['dphi2'],\n",
    "                'domega2': blockresult._BlockResults__unblinded.result['domega2'],\n",
    "                'dtau2': blockresult._BlockResults__unblinded.result['dtau2'],\n",
    "                'N': blockresult._BlockResults__unblinded.result['N'],\n",
    "                'A': blockresult._BlockResults__unblinded.result['A'],\n",
    "                'dA2': blockresult._BlockResults__unblinded.result['dA2'],\n",
    "                'xsipm': blockresult._BlockResults__unblinded.result['xsipm'],\n",
    "                'ysipm': blockresult._BlockResults__unblinded.result['ysipm'],\n",
    "                'zsipm': blockresult._BlockResults__unblinded.result['zsipm'],\n",
    "                'Ap': blockresult._BlockResults__unblinded.result['Ap'],\n",
    "                'dAp2': blockresult._BlockResults__unblinded.result['dAp2'],\n",
    "                'Am': blockresult._BlockResults__unblinded.result['Am'],\n",
    "                'dAm2': blockresult._BlockResults__unblinded.result['dAm2'],\n",
    "                'C_summary': blockresult._BlockResults__unblinded.result_summary['C_summary'],\n",
    "                'phi_summary': blockresult._BlockResults__unblinded.result_summary['phi_summary'],\n",
    "                'omega_summary': blockresult._BlockResults__unblinded.result_summary['omega_summary'],\n",
    "                'tau_summary': blockresult._BlockResults__unblinded.result_summary['tau_summary'],\n",
    "                'dC2_summary': blockresult._BlockResults__unblinded.result_summary['dC2_summary'],\n",
    "                'dphi2_summary': blockresult._BlockResults__unblinded.result_summary['dphi2_summary'],\n",
    "                'domega2_summary': blockresult._BlockResults__unblinded.result_summary['domega2_summary'],\n",
    "                'dtau2_summary': blockresult._BlockResults__unblinded.result_summary['dtau2_summary'],\n",
    "                'N_summary': blockresult._BlockResults__unblinded.result_summary['N_summary'],\n",
    "                'A_summary': blockresult._BlockResults__unblinded.result_summary['A_summary'],\n",
    "                'dA2_summary': blockresult._BlockResults__unblinded.result_summary['dA2_summary'],\n",
    "                'xsipm_summary': blockresult._BlockResults__unblinded.result_summary['xsipm_summary'],\n",
    "                'ysipm_summary': blockresult._BlockResults__unblinded.result_summary['ysipm_summary'],\n",
    "                'zsipm_summary': blockresult._BlockResults__unblinded.result_summary['zsipm_summary'],\n",
    "                'Ap_summary': blockresult._BlockResults__unblinded.result_summary['Ap_summary'],\n",
    "                'dAp2_summary': blockresult._BlockResults__unblinded.result_summary['dAp2_summary'],\n",
    "                'Am_summary': blockresult._BlockResults__unblinded.result_summary['Am_summary'],\n",
    "                'dAm2_summary': blockresult._BlockResults__unblinded.result_summary['dAm2_summary']\n",
    "            }\n",
    "\n",
    "            # Extract non-parity and superblock parity switch values from self.df\n",
    "            temp_df = self.df[(self.df['run'] == run) & (self.df['sequence'] == sequence) & (self.df['block'] == block)]\n",
    "            \n",
    "            # Get the tuple for non_parity_switches\n",
    "            if self.non_parity_switches:\n",
    "                non_parity_tuple = tuple(temp_df[sw].values[0] for sw in self.non_parity_switches)\n",
    "            else:\n",
    "                non_parity_tuple = ()\n",
    "\n",
    "            # Get the tuple for superblock_parity_switches\n",
    "            if self.superblock_parity_switches:\n",
    "                superblock_parity_tuple = tuple(temp_df[sw].values[0] for sw in self.superblock_parity_switches)\n",
    "            else:\n",
    "                superblock_parity_tuple = ()\n",
    "\n",
    "            block_df_old_columns = block_df.columns \n",
    "\n",
    "            for idx, label in enumerate(self.superblock_parity_switches):\n",
    "                block_df[label] = superblock_parity_tuple[idx]\n",
    "            \n",
    "            for idx, label in enumerate(self.non_parity_switches):\n",
    "                block_df[label] = non_parity_tuple[idx]\n",
    "\n",
    "            # move the superblock_parity_tuple to the front\n",
    "            block_df = block_df[[\"run\", \"sequence\", \"block\"] + self.non_parity_switches + self.superblock_parity_switches + list(block_df_old_columns[3:])]\n",
    "            self.sequenceresult.blockdf.append(block_df)\n",
    "\n",
    "            # Populate the blockdict with the arrays from blockresult\n",
    "            for quantity, array in quantities.items():\n",
    "                if quantity not in self.blockdict:\n",
    "                    self.blockdict[quantity] = {}\n",
    "\n",
    "                if non_parity_tuple not in self.blockdict[quantity]:\n",
    "                    self.blockdict[quantity][non_parity_tuple] = {}\n",
    "\n",
    "                if superblock_parity_tuple not in self.blockdict[quantity][non_parity_tuple]:\n",
    "                    self.blockdict[quantity][non_parity_tuple][superblock_parity_tuple] = []\n",
    "\n",
    "                # Append the array to the corresponding dictionary entry\n",
    "                self.blockdict[quantity][non_parity_tuple][superblock_parity_tuple].append(array)\n",
    "        self.sequenceresult.blockdf = pd.concat(self.sequenceresult.blockdf, ignore_index=True)\n",
    "\n",
    "        self.sequenceresult.sequence_group_left = self.sequence_begin_group\n",
    "        self.sequenceresult.sequence_group_right = self.sequence_end_group\n",
    "\n",
    "    def _diag_along_axis(A, axis):\n",
    "        shape = A.shape\n",
    "        new_shape = list(shape)\n",
    "        new_shape.insert(axis, shape[axis])\n",
    "        \n",
    "        diag_mask = np.eye(shape[axis], dtype=A.dtype)\n",
    "        expanded_diag_shape = [1] * (len(shape) + 1)\n",
    "        expanded_diag_shape[axis] = shape[axis]\n",
    "        expanded_diag_shape[axis + 1] = shape[axis]\n",
    "        diag_mask = diag_mask.reshape(expanded_diag_shape)\n",
    "        \n",
    "        expanded_A_shape = list(shape)\n",
    "        expanded_A_shape.insert(axis, 1)\n",
    "        expanded_A = A.reshape(expanded_A_shape)\n",
    "        \n",
    "        B = diag_mask * expanded_A\n",
    "        \n",
    "        return B\n",
    "\n",
    "    def _extract_diagonal_along_axes(matrix, axis):\n",
    "        \"\"\"\n",
    "        Extract the diagonal elements along the specified axis and the next axis,\n",
    "        and bring the diagonal axis to the position of the specified axis.\n",
    "\n",
    "        Parameters:\n",
    "        matrix (np.ndarray): The input n-dimensional array.\n",
    "        axis (int): The axis along which to extract diagonals with the next axis.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray: The resultant array after extracting diagonals.\n",
    "        \"\"\"\n",
    "        if axis < 0 or axis >= matrix.ndim - 1:\n",
    "            raise ValueError(\"Axis out of bounds or too high for the given matrix dimensions.\")\n",
    "        \n",
    "        # Extract the diagonal along the specified axis and the next axis\n",
    "        diag_matrix = np.diagonal(matrix, axis1=axis, axis2=axis + 1)\n",
    "        \n",
    "        # Move the diagonal axis to the original position of the specified axis\n",
    "        new_axes_order = list(range(diag_matrix.ndim))\n",
    "        new_axes_order.insert(axis, new_axes_order.pop(-1))\n",
    "        \n",
    "        result = np.transpose(diag_matrix, new_axes_order)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def parity_transformation_for_value(self, matrix):\n",
    "        return np.tensordot(self.P,  matrix, axes=(1, 0))\n",
    "\n",
    "    def parity_transformation_for_variance(self, matrix):\n",
    "        sigma_state = sequenceCalculator._diag_along_axis(matrix, 0)\n",
    "        result = np.tensordot(self.P, np.moveaxis(np.tensordot(sigma_state, self.P.T, axes=([1], [0])),-1,1), axes=([1], [0]))\n",
    "        return sequenceCalculator._extract_diagonal_along_axes(result, 0)\n",
    "\n",
    "    def _propagate_error_bar(A, dA2, axis_to_take_average, nan = False):\n",
    "        if nan:\n",
    "            if isinstance(axis_to_take_average, int):\n",
    "                axis_to_take_average = (axis_to_take_average,)\n",
    "\n",
    "            selected_dimension = [A.shape[i] for i in axis_to_take_average]\n",
    "\n",
    "            if np.prod(selected_dimension) == 1:\n",
    "                A_mean = A\n",
    "                dA_mean_square_scaled = dA2\n",
    "                red_chi_square = np.full(A.shape, np.nan)\n",
    "                dA_mean_square_unscaled = dA2\n",
    "                return A_mean, dA_mean_square_unscaled, dA_mean_square_scaled, red_chi_square\n",
    "\n",
    "            weights = 1 / dA2\n",
    "            A_mean = np.nansum(A * weights, axis=axis_to_take_average, keepdims=True) / np.nansum(weights, axis=axis_to_take_average, keepdims=True)\n",
    "\n",
    "            dA_mean_square_unscaled = 1 / np.nansum(weights, axis=axis_to_take_average, keepdims=True)\n",
    "\n",
    "            total_number_of_points = np.nanprod([A.shape[axis] for axis in axis_to_take_average])\n",
    "            residuals = (A - A_mean)**2\n",
    "            red_chi_square = np.nansum(residuals / dA2, axis=axis_to_take_average, keepdims=True) / (total_number_of_points - 1)\n",
    "\n",
    "            dA_mean_square_scaled = dA_mean_square_unscaled * red_chi_square\n",
    "\n",
    "            return A_mean, dA_mean_square_unscaled, dA_mean_square_scaled, red_chi_square\n",
    "        \n",
    "        else:\n",
    "\n",
    "            if isinstance(axis_to_take_average, int):\n",
    "                axis_to_take_average = (axis_to_take_average,)\n",
    "\n",
    "            selected_dimension = [A.shape[i] for i in axis_to_take_average]\n",
    "\n",
    "            if np.prod(selected_dimension) == 1:\n",
    "                A_mean = A\n",
    "                dA_mean_square_scaled = dA2\n",
    "                red_chi_square = np.full(A.shape, np.nan)\n",
    "                dA_mean_square_unscaled = dA2\n",
    "                return A_mean, dA_mean_square_unscaled, dA_mean_square_scaled, red_chi_square\n",
    "\n",
    "            weights = 1 / dA2\n",
    "            A_mean = np.sum(A * weights, axis=axis_to_take_average, keepdims=True) / np.sum(weights, axis=axis_to_take_average, keepdims=True)\n",
    "\n",
    "            dA_mean_square_unscaled = 1 / np.sum(weights, axis=axis_to_take_average, keepdims=True)\n",
    "\n",
    "            total_number_of_points = np.prod([A.shape[axis] for axis in axis_to_take_average])\n",
    "            residuals = (A - A_mean)**2\n",
    "            red_chi_square = np.sum(residuals / dA2, axis=axis_to_take_average, keepdims=True) / (total_number_of_points - 1)\n",
    "\n",
    "            dA_mean_square_scaled = dA_mean_square_unscaled * red_chi_square\n",
    "\n",
    "            return A_mean, dA_mean_square_unscaled, dA_mean_square_scaled, red_chi_square\n",
    "        \n",
    "    def _create_superblock_state_dict(self):\n",
    "        # Initialize the superblock_state_dict\n",
    "        self.superblock_state_dict = {}\n",
    "\n",
    "        # List of quantity pairs for error propagation\n",
    "        quantity_pairs = [\n",
    "            ('C', 'dC2', 'degen_block_red_chi_square_C'),\n",
    "            ('phi', 'dphi2', 'degen_block_red_chi_square_phi'),\n",
    "            ('tau', 'dtau2', 'degen_block_red_chi_square_tau'),\n",
    "            ('omega', 'domega2', 'degen_block_red_chi_square_omega'),\n",
    "            ('C_summary', 'dC2_summary', 'degen_block_red_chi_square_C_summary'),\n",
    "            ('phi_summary', 'dphi2_summary', 'degen_block_red_chi_square_phi_summary'),\n",
    "            ('tau_summary', 'dtau2_summary', 'degen_block_red_chi_square_tau_summary'),\n",
    "            ('omega_summary', 'domega2_summary', 'degen_block_red_chi_square_omega_summary'),\n",
    "            ('A', 'dA2', 'degen_block_red_chi_square_A'),\n",
    "            ('Ap', 'dAp2', 'degen_block_red_chi_square_Ap'),\n",
    "            ('Am', 'dAm2', 'degen_block_red_chi_square_Am'),\n",
    "            ('A_summary', 'dA2_summary', 'degen_block_red_chi_square_A_summary'),\n",
    "            ('Ap_summary', 'dAp2_summary', 'degen_block_red_chi_square_Ap_summary'),\n",
    "            ('Am_summary', 'dAm2_summary', 'degen_block_red_chi_square_Am_summary')\n",
    "        ]\n",
    "\n",
    "        # Process the blockdict for each quantity\n",
    "        for quantity1, quantity2, red_chi_square_quantity in quantity_pairs:\n",
    "            if quantity1 not in self.superblock_state_dict:\n",
    "                self.superblock_state_dict[quantity1] = {}\n",
    "            if quantity2 not in self.superblock_state_dict:\n",
    "                self.superblock_state_dict[quantity2] = {}\n",
    "            if red_chi_square_quantity not in self.superblock_state_dict:\n",
    "                self.superblock_state_dict[red_chi_square_quantity] = {}\n",
    "\n",
    "            # Iterate over keys for non_parity_switches\n",
    "            for non_parity_key in self.blockdict[quantity1].keys():\n",
    "                if non_parity_key not in self.superblock_state_dict[quantity1]:\n",
    "                    self.superblock_state_dict[quantity1][non_parity_key] = {}\n",
    "                    self.superblock_state_dict[quantity2][non_parity_key] = {}\n",
    "                    self.superblock_state_dict[red_chi_square_quantity][non_parity_key] = {}\n",
    "\n",
    "                # Iterate over keys for superblock_parity_switches\n",
    "                for superblock_parity_key in self.blockdict[quantity1][non_parity_key].keys():\n",
    "                    # Stack arrays for quantity1 and quantity2 along axis 0\n",
    "                    A_list = self.blockdict[quantity1][non_parity_key][superblock_parity_key]\n",
    "                    dA2_list = self.blockdict[quantity2][non_parity_key][superblock_parity_key]\n",
    "                    \n",
    "                    A_stacked = np.stack(A_list, axis=0)\n",
    "                    dA2_stacked = np.stack(dA2_list, axis=0)\n",
    "\n",
    "                    # Apply error propagation\n",
    "                    A_mean, dA_mean_square_unscaled, _, red_chi_square = sequenceCalculator._propagate_error_bar(A_stacked, dA2_stacked, axis_to_take_average=0, nan=False)\n",
    "\n",
    "                    # Store results in superblock_state_dict\n",
    "                    self.superblock_state_dict[quantity1][non_parity_key][superblock_parity_key] = A_mean\n",
    "                    self.superblock_state_dict[quantity2][non_parity_key][superblock_parity_key] = dA_mean_square_unscaled\n",
    "                    self.superblock_state_dict[red_chi_square_quantity][non_parity_key][superblock_parity_key] = red_chi_square\n",
    "\n",
    "        # Now handle 'N' and 'N_summary' by summing the arrays\n",
    "        for quantity in ['N', 'N_summary']:\n",
    "            if quantity not in self.superblock_state_dict:\n",
    "                self.superblock_state_dict[quantity] = {}\n",
    "\n",
    "            # Iterate over keys for non_parity_switches\n",
    "            for non_parity_key in self.blockdict[quantity].keys():\n",
    "                if non_parity_key not in self.superblock_state_dict[quantity]:\n",
    "                    self.superblock_state_dict[quantity][non_parity_key] = {}\n",
    "\n",
    "                # Iterate over keys for superblock_parity_switches\n",
    "                for superblock_parity_key in self.blockdict[quantity][non_parity_key].keys():\n",
    "                    N_list = self.blockdict[quantity][non_parity_key][superblock_parity_key]\n",
    "                    N_summed = np.sum(N_list, axis=0, keepdims=True)\n",
    "\n",
    "                    # Store summed result in superblock_state_dict\n",
    "                    self.superblock_state_dict[quantity][non_parity_key][superblock_parity_key] = N_summed\n",
    "\n",
    "        # Now handle 'xsipm' and 'xsipm_summary' by averaging the arrays\n",
    "        for quantity in ['xsipm', 'xsipm_summary']:\n",
    "            if quantity not in self.superblock_state_dict:\n",
    "                self.superblock_state_dict[quantity] = {}\n",
    "\n",
    "            # Iterate over keys for non_parity_switches\n",
    "            for non_parity_key in self.blockdict[quantity].keys():\n",
    "                if non_parity_key not in self.superblock_state_dict[quantity]:\n",
    "                    self.superblock_state_dict[quantity][non_parity_key] = {}\n",
    "\n",
    "                # Iterate over keys for superblock_parity_switches\n",
    "                for superblock_parity_key in self.blockdict[quantity][non_parity_key].keys():\n",
    "                    xsipm_list = self.blockdict[quantity][non_parity_key][superblock_parity_key]\n",
    "                    xsipm_mean = np.mean(xsipm_list, axis=0, keepdims=True)\n",
    "\n",
    "                    # Store averaged result in superblock_state_dict\n",
    "                    self.superblock_state_dict[quantity][non_parity_key][superblock_parity_key] = xsipm_mean\n",
    "        \n",
    "        # Now handle 'ysipm' and 'ysipm_summary' by averaging the arrays\n",
    "        for quantity in ['ysipm', 'ysipm_summary']:\n",
    "            if quantity not in self.superblock_state_dict:\n",
    "                self.superblock_state_dict[quantity] = {}\n",
    "\n",
    "            # Iterate over keys for non_parity_switches\n",
    "            for non_parity_key in self.blockdict[quantity].keys():\n",
    "                if non_parity_key not in self.superblock_state_dict[quantity]:\n",
    "                    self.superblock_state_dict[quantity][non_parity_key] = {}\n",
    "\n",
    "                # Iterate over keys for superblock_parity_switches\n",
    "                for superblock_parity_key in self.blockdict[quantity][non_parity_key].keys():\n",
    "                    ysipm_list = self.blockdict[quantity][non_parity_key][superblock_parity_key]\n",
    "                    ysipm_mean = np.mean(ysipm_list, axis=0, keepdims=True)\n",
    "\n",
    "                    # Store averaged result in superblock_state_dict\n",
    "                    self.superblock_state_dict[quantity][non_parity_key][superblock_parity_key] = ysipm_mean\n",
    "\n",
    "        # Now handle 'zsipm' and 'zsipm_summary' by averaging the arrays\n",
    "        for quantity in ['zsipm', 'zsipm_summary']:\n",
    "            if quantity not in self.superblock_state_dict:\n",
    "                self.superblock_state_dict[quantity] = {}\n",
    "\n",
    "            # Iterate over keys for non_parity_switches\n",
    "            for non_parity_key in self.blockdict[quantity].keys():\n",
    "                if non_parity_key not in self.superblock_state_dict[quantity]:\n",
    "                    self.superblock_state_dict[quantity][non_parity_key] = {}\n",
    "\n",
    "                # Iterate over keys for superblock_parity_switches\n",
    "                for superblock_parity_key in self.blockdict[quantity][non_parity_key].keys():\n",
    "                    zsipm_list = self.blockdict[quantity][non_parity_key][superblock_parity_key]\n",
    "                    zsipm_mean = np.mean(zsipm_list, axis=0, keepdims=True)\n",
    "\n",
    "                    # Store averaged result in superblock_state_dict\n",
    "                    self.superblock_state_dict[quantity][non_parity_key][superblock_parity_key] = zsipm_mean\n",
    "\n",
    "    def _create_sequence_results(self):\n",
    "        # Initialize sequence_result and sequence_result_summary dictionaries\n",
    "        self.sequence_result = {}\n",
    "        self.sequence_result_summary = {}\n",
    "        self.final_result = {}\n",
    "\n",
    "        # Explicitly list the quantities for sequence_result and sequence_result_summary\n",
    "        quantities_for_value_transformation = ['C', 'phi', 'omega', 'tau', 'N', 'A', 'xsipm', 'ysipm', 'zsipm','Ap', 'Am']\n",
    "        quantities_for_value_transformation_summary = ['C_summary', 'phi_summary', 'omega_summary', 'tau_summary', 'N_summary','A_summary', 'xsipm_summary', 'ysipm_summary', 'zsipm_summary','Ap_summary', 'Am_summary']\n",
    "\n",
    "\n",
    "        quantities_for_variance_transformation = ['dC2', 'dphi2', 'domega2', 'dtau2', 'dA2', 'dAp2', 'dAm2']\n",
    "        quantities_for_variance_transformation_summary = ['dC2_summary', 'dphi2_summary', 'domega2_summary', 'dtau2_summary', 'dA2_summary','dAp2_summary', 'dAm2_summary']\n",
    "\n",
    "\n",
    "        failed_non_parity_keys = []\n",
    "\n",
    "        # Process the superblock_state_dict for each quantity\n",
    "        for quantity in self.superblock_state_dict:\n",
    "            # Determine whether the quantity belongs to sequence_result or sequence_result_summary\n",
    "            if quantity in quantities_for_value_transformation_summary:\n",
    "                result_dict = self.sequence_result_summary\n",
    "            elif quantity in quantities_for_variance_transformation_summary:\n",
    "                result_dict = self.sequence_result_summary\n",
    "            elif quantity in quantities_for_value_transformation:\n",
    "                result_dict = self.sequence_result\n",
    "            elif quantity in quantities_for_variance_transformation:\n",
    "                result_dict = self.sequence_result\n",
    "            else:\n",
    "                continue  # Skip if the quantity is not listed explicitly\n",
    "\n",
    "            # Initialize the dictionary for this quantity in the result_dict\n",
    "            if quantity not in result_dict:\n",
    "                result_dict[quantity] = {}\n",
    "\n",
    "            # Iterate over non_parity_switches keys\n",
    "            for non_parity_key in self.superblock_state_dict[quantity].keys():\n",
    "                # Get the superblock parity switch states as a sorted list of tuples in descending order\n",
    "                superblock_keys_sorted = sorted(self.superblock_state_dict[quantity][non_parity_key].keys(), reverse=True)\n",
    "\n",
    "                # Stack all matrices corresponding to these superblock states along a new axis (axis 0)\n",
    "                stacked_matrix = np.stack(\n",
    "                    [self.superblock_state_dict[quantity][non_parity_key][superblock_key] for superblock_key in superblock_keys_sorted],\n",
    "                    axis=0\n",
    "                )\n",
    "\n",
    "                # Apply the necessary transformations based on the quantity\n",
    "                if quantity in quantities_for_value_transformation or quantity in quantities_for_value_transformation_summary:\n",
    "                    try:\n",
    "                        transformed_matrix = self.parity_transformation_for_value(stacked_matrix)\n",
    "                    except Exception as e:\n",
    "                        failed_non_parity_keys.append(non_parity_key)\n",
    "                        continue\n",
    "                elif quantity in quantities_for_variance_transformation or quantity in quantities_for_variance_transformation_summary:\n",
    "                    try:\n",
    "                        transformed_matrix = self.parity_transformation_for_variance(stacked_matrix)\n",
    "                    except Exception as e:\n",
    "                        failed_non_parity_keys.append(non_parity_key)\n",
    "                        continue\n",
    "                else:\n",
    "                    transformed_matrix = stacked_matrix  # No transformation needed for other quantities\n",
    "\n",
    "                # Store the transformed matrix in the result dictionary\n",
    "                result_dict[quantity][non_parity_key] = transformed_matrix\n",
    "\n",
    "        # Remove failed non_parity_keys from the sequence_result and sequence_result_summary dictionaries\n",
    "        for non_parity_key in failed_non_parity_keys:\n",
    "            for quantity in self.sequence_result:\n",
    "                if non_parity_key in self.sequence_result[quantity]:\n",
    "                    del self.sequence_result[quantity][non_parity_key]\n",
    "            for quantity in self.sequence_result_summary:\n",
    "                if non_parity_key in self.sequence_result_summary[quantity]:\n",
    "                    del self.sequence_result_summary[quantity][non_parity_key]\n",
    "\n",
    "        # Now generate the final result\n",
    "        for quantity_value, quantity_variance in [\n",
    "            ('C_summary', 'dC2_summary'),\n",
    "            ('phi_summary', 'dphi2_summary'),\n",
    "            ('omega_summary', 'domega2_summary'),\n",
    "            ('tau_summary', 'dtau2_summary'),\n",
    "            ('A_summary', 'dA2_summary'),\n",
    "            ('Ap_summary', 'dAp2_summary'),\n",
    "            ('Am_summary', 'dAm2_summary')\n",
    "        ]:\n",
    "            if quantity_value in self.sequence_result_summary and quantity_variance in self.sequence_result_summary:\n",
    "                for non_parity_key in self.sequence_result_summary[quantity_value]:\n",
    "                    A_summary = self.sequence_result_summary[quantity_value][non_parity_key]\n",
    "                    dA2_summary = self.sequence_result_summary[quantity_variance][non_parity_key]\n",
    "\n",
    "                    # Apply error propagation on axis -2\n",
    "                    A_mean, dA_mean_square_unscaled, _, red_chi_square = sequenceCalculator._propagate_error_bar(A_summary, dA2_summary, axis_to_take_average=-2, nan=False)\n",
    "\n",
    "                    # Store the results in final_result, removing the '_summary' suffix\n",
    "                    quantity = quantity_value.replace('_summary', '')\n",
    "                    variance_quantity = quantity_variance.replace('_summary', '')\n",
    "                    self.final_result[quantity] = self.final_result.get(quantity, {})\n",
    "                    self.final_result[variance_quantity] = self.final_result.get(variance_quantity, {})\n",
    "                    self.final_result[f'sipm_red_chi_square_{quantity}'] = self.final_result.get(f'sipm_red_chi_square_{quantity}', {})\n",
    "\n",
    "                    self.final_result[quantity][non_parity_key] = A_mean\n",
    "                    self.final_result[variance_quantity][non_parity_key] = dA_mean_square_unscaled\n",
    "                    self.final_result[f'sipm_red_chi_square_{quantity}'][non_parity_key] = red_chi_square\n",
    "\n",
    "        # Sum 'N_summary' along axis -2 and store in final_result as 'N'\n",
    "        if 'N_summary' in self.sequence_result_summary:\n",
    "            for non_parity_key in self.sequence_result_summary['N_summary']:\n",
    "                N_summary = self.sequence_result_summary['N_summary'][non_parity_key]\n",
    "                N_summed = np.sum(N_summary, axis=-2, keepdims = True)\n",
    "                self.final_result['N'] = self.final_result.get('N', {})\n",
    "                self.final_result['N'][non_parity_key] = N_summed\n",
    "                \n",
    "        # store xsipm, ysipm, zsipm _ summary in final result:\n",
    "        for quantity in ['xsipm_summary', 'ysipm_summary', 'zsipm_summary']:\n",
    "            if quantity in self.sequence_result_summary:\n",
    "                for non_parity_key in self.sequence_result_summary[quantity]:\n",
    "                    self.final_result[quantity] = self.final_result.get(quantity, {})\n",
    "                    self.final_result[quantity][non_parity_key] = self.sequence_result_summary[quantity][non_parity_key]\n",
    "\n",
    "    def _deep_copy_results(self):\n",
    "        # Deep copy sequence_result, sequence_result_summary, and superblock_state_dict into __unblinded and blinded\n",
    "        self.sequenceresult.blinded.result = copy.deepcopy(self.sequence_result)\n",
    "        self.sequenceresult.blinded.result_summary = copy.deepcopy(self.sequence_result_summary)\n",
    "        self.sequenceresult.blinded.superblock_state_result = copy.deepcopy(self.superblock_state_dict)\n",
    "        self.sequenceresult.blinded.final_result = copy.deepcopy(self.final_result)\n",
    "\n",
    "        # Access the protected __unblinded attribute correctly\n",
    "        self.sequenceresult._SequenceResults__unblinded.result = copy.deepcopy(self.sequence_result)\n",
    "        self.sequenceresult._SequenceResults__unblinded.result_summary = copy.deepcopy(self.sequence_result_summary)\n",
    "        self.sequenceresult._SequenceResults__unblinded.superblock_state_result = copy.deepcopy(self.superblock_state_dict)\n",
    "        self.sequenceresult._SequenceResults__unblinded.final_result = copy.deepcopy(self.final_result)\n",
    "\n",
    "    def _apply_blinding(self):\n",
    "        # Blinding for superblock_state_result (handling both quantity and quantity_summary)\n",
    "        for quantity in ['omega', 'phi', 'omega_summary', 'phi_summary', 'A', 'A_summary']:\n",
    "            if quantity in self.sequenceresult.blinded.superblock_state_result:\n",
    "                for non_parity_key in self.sequenceresult.blinded.superblock_state_result[quantity].keys():\n",
    "                    for superblock_key in self.sequenceresult.blinded.superblock_state_result[quantity][non_parity_key]:\n",
    "                        matrix = self.sequenceresult.blinded.superblock_state_result[quantity][non_parity_key][superblock_key]\n",
    "\n",
    "                        if 'omega' == quantity:\n",
    "                            # Shift omega and omega_summary[:, 4, ...] by blind value\n",
    "                            matrix[:, 4, ...] += self.blind._blind_value_in_rad_s\n",
    "                        elif 'phi' == quantity:\n",
    "                            # Shift phi and phi_summary[:, 4, ...] by tau[:, 0, ...] * blind value\n",
    "                            tau_matrix = self.sequenceresult.blinded.superblock_state_result['tau'][non_parity_key][superblock_key]\n",
    "                            matrix[:, 4, ...] += tau_matrix[:, 0, ...] * self.blind._blind_value_in_rad_s\n",
    "                        elif 'A' == quantity:\n",
    "                            # Shift phi and phi_summary[:, 4, ...] by tau[:, 0, ...] * blind value\n",
    "                            tau_matrix = self.sequenceresult.blinded.superblock_state_result['tau'][non_parity_key][superblock_key]\n",
    "                            matrix[:, 4, ...] += tau_matrix[:, 0, ...] * self.blind._blind_value_in_rad_s * 2\n",
    "                        elif 'omega_summary' == quantity:\n",
    "                            matrix[:, 4, ...] += self.blind._blind_value_in_rad_s\n",
    "                        elif 'phi_summary' == quantity:\n",
    "                            tau_matrix = self.sequenceresult.blinded.superblock_state_result['tau_summary'][non_parity_key][superblock_key]\n",
    "                            matrix[:, 4, ...] += tau_matrix[:, 0, ...] * self.blind._blind_value_in_rad_s\n",
    "                        elif 'A_summary' == quantity:\n",
    "                            tau_matrix = self.sequenceresult.blinded.superblock_state_result['tau_summary'][non_parity_key][superblock_key]\n",
    "                            matrix[:, 4, ...] += tau_matrix[:, 0, ...] * self.blind._blind_value_in_rad_s * 2\n",
    "\n",
    "\n",
    "        # Blinding for result and result_summary\n",
    "        for quantity in ['omega', 'phi', 'A']:\n",
    "            if quantity in self.sequenceresult.blinded.result:\n",
    "                for non_parity_key in self.sequenceresult.blinded.result[quantity].keys():\n",
    "                    matrix = self.sequenceresult.blinded.result[quantity][non_parity_key]\n",
    "\n",
    "                    if quantity == 'omega':\n",
    "                        # Shift omega[0, :, 4, ...] by blind value\n",
    "                        matrix[0, :, 4, ...] += self.blind._blind_value_in_rad_s\n",
    "                    elif quantity == 'phi':\n",
    "                        # Shift phi[0, :, 4, ...] by tau[0, :, 0, ...] * blind value\n",
    "                        tau_matrix = self.sequenceresult.blinded.result['tau'][non_parity_key]\n",
    "                        matrix[0, :, 4, ...] += tau_matrix[0, :, 0, ...] * self.blind._blind_value_in_rad_s\n",
    "                    elif quantity == 'A':\n",
    "                        # Shift phi[0, :, 4, ...] by tau[0, :, 0, ...] * blind value\n",
    "                        tau_matrix = self.sequenceresult.blinded.result['tau'][non_parity_key]\n",
    "                        matrix[0, :, 4, ...] += tau_matrix[0, :, 0, ...] * self.blind._blind_value_in_rad_s * 2\n",
    "\n",
    "        # Blinding for result_summary (same as result)\n",
    "        for quantity in ['omega_summary', 'phi_summary', 'A_summary']:\n",
    "            if quantity in self.sequenceresult.blinded.result_summary:\n",
    "                for non_parity_key in self.sequenceresult.blinded.result_summary[quantity].keys():\n",
    "                    matrix = self.sequenceresult.blinded.result_summary[quantity][non_parity_key]\n",
    "\n",
    "                    if quantity == 'omega_summary':\n",
    "                        # Shift omega_summary[0, :, 4, ...] by blind value\n",
    "                        matrix[0, :, 4, ...] += self.blind._blind_value_in_rad_s\n",
    "                    elif quantity == 'phi_summary':\n",
    "                        # Shift phi_summary[0, :, 4, ...] by tau_summary[0, :, 0, ...] * blind value\n",
    "                        tau_matrix = self.sequenceresult.blinded.result_summary['tau_summary'][non_parity_key]\n",
    "                        matrix[0, :, 4, ...] += tau_matrix[0, :, 0, ...] * self.blind._blind_value_in_rad_s\n",
    "                    elif quantity == 'A_summary':\n",
    "                        # Shift phi_summary[0, :, 4, ...] by tau_summary[0, :, 0, ...] * blind value\n",
    "                        tau_matrix = self.sequenceresult.blinded.result_summary['tau_summary'][non_parity_key]\n",
    "                        matrix[0, :, 4, ...] += tau_matrix[0, :, 0, ...] * self.blind._blind_value_in_rad_s * 2\n",
    "\n",
    "        # Blinding for final_result\n",
    "        for quantity in ['omega', 'phi', 'A']:\n",
    "            if quantity in self.sequenceresult.blinded.final_result:\n",
    "                for non_parity_key in self.sequenceresult.blinded.final_result[quantity]:\n",
    "                    matrix = self.sequenceresult.blinded.final_result[quantity][non_parity_key]\n",
    "\n",
    "                    if quantity == 'omega':\n",
    "                        # Shift omega[:, 4, ...] by blind value\n",
    "                        matrix[0, :, 4, ...] += self.blind._blind_value_in_rad_s\n",
    "                    elif quantity == 'phi':\n",
    "                        # Shift phi[:, 4, ...] by tau[:, 0, ...] * blind value\n",
    "                        tau_matrix = self.sequenceresult.blinded.final_result['tau'][non_parity_key]\n",
    "                        matrix[0 ,:, 4, ...] += tau_matrix[0, :, 0, ...] * self.blind._blind_value_in_rad_s\n",
    "                    elif quantity == 'A':\n",
    "                        # Shift phi[:, 4, ...] by tau[:, 0, ...] * blind value\n",
    "                        tau_matrix = self.sequenceresult.blinded.final_result['tau'][non_parity_key]\n",
    "                        matrix[0 ,:, 4, ...] += tau_matrix[0, :, 0, ...] * self.blind._blind_value_in_rad_s * 2\n",
    "\n",
    "    def _block_df_combine_sipm(superblock_parity_switches, non_parity_switches, df):\n",
    "        # Define the quantities to average and channels to process\n",
    "        quantity_for_average = [\"C\", \"phi\", \"omega\", \"tau\", \"A\", \"Ap\", \"Am\"]\n",
    "        channels = [\"nr\", \"N\", \"E\", \"B\", \"NE\", \"NB\", \"EB\", \"NEB\"]\n",
    "\n",
    "        # Function to calculate inverse-variance weighted average\n",
    "        def weighted_average_with_uncertainty(df, quantity, channel):\n",
    "            sipm_columns = [f'block_{quantity}_{channel}_sipm{i}' for i in range(8) if f'block_{quantity}_{channel}_sipm{i}' in df.columns]\n",
    "            uncertainty_columns = [f'blockuncertainty_{quantity}_{channel}_sipm{i}' for i in range(8) if f'blockuncertainty_{quantity}_{channel}_sipm{i}' in df.columns]\n",
    "            \n",
    "            values = df[sipm_columns].values\n",
    "            uncertainties = df[uncertainty_columns].values\n",
    "            \n",
    "            # Inverse of variance (uncertainty squared)\n",
    "            weights = 1 / (uncertainties ** 2)\n",
    "            \n",
    "            # Weighted average\n",
    "            weighted_avg = np.sum(values * weights, axis=1) / np.sum(weights, axis=1)\n",
    "            \n",
    "            # New uncertainty (harmonic sum)\n",
    "            new_uncertainty = np.sqrt(1 / np.sum(weights, axis=1))\n",
    "            \n",
    "            # Add the new columns back to the original DataFrame\n",
    "            df[f'block_{quantity}_{channel}'] = weighted_avg\n",
    "            df[f'blockuncertainty_{quantity}_{channel}'] = new_uncertainty\n",
    "\n",
    "        # Function to sum the block_N values for a channel\n",
    "        def sum_N_values(df, channel):\n",
    "            sipm_columns = [f'block_N_{channel}_sipm{i}' for i in range(8) if f'block_N_{channel}_sipm{i}' in df.columns]\n",
    "            \n",
    "            # Sum values across SiPMs\n",
    "            summed_values = df[sipm_columns].sum(axis=1)\n",
    "            \n",
    "            # Add the new column back to the original DataFrame\n",
    "            df[f'block_N_{channel}'] = summed_values\n",
    "\n",
    "        def average_zsipm_values(df, channel):\n",
    "            sipm_columns = [f'block_zsipm_{channel}_sipm{i}' for i in range(8) if f'block_zsipm_{channel}_sipm{i}' in df.columns]\n",
    "            averaged_values = df[sipm_columns].mean(axis=1)\n",
    "            df[f'block_zsipm_{channel}'] = averaged_values\n",
    "        \n",
    "        def average_ysipm_values(df, channel):\n",
    "            sipm_columns = [f'block_ysipm_{channel}_sipm{i}' for i in range(8) if f'block_ysipm_{channel}_sipm{i}' in df.columns]\n",
    "            averaged_values = df[sipm_columns].mean(axis=1)\n",
    "            df[f'block_ysipm_{channel}'] = averaged_values\n",
    "\n",
    "        def average_xsipm_values(df, channel):\n",
    "            sipm_columns = [f'block_xsipm_{channel}_sipm{i}' for i in range(8) if f'block_xsipm_{channel}_sipm{i}' in df.columns]\n",
    "            averaged_values = df[sipm_columns].mean(axis=1)\n",
    "            df[f'block_xsipm_{channel}'] = averaged_values\n",
    "\n",
    "        # Process all quantities except 'N' quantities for all channels\n",
    "        for quantity in quantity_for_average:\n",
    "            for channel in channels:\n",
    "                if quantity == 'tau' and channel != 'nr':  # Exception for 'tau'\n",
    "                    continue\n",
    "                if quantity != 'N' and quantity != 'zsipm' and quantity != 'ysipm' and quantity != 'xsipm':  # Avoid processing 'N' , 'zsipm', 'xsipm', 'ysipm'quantities\n",
    "                    weighted_average_with_uncertainty(df, quantity, channel)\n",
    "\n",
    "        # Process 'N' quantities by summing values without uncertainties\n",
    "        for channel in channels:\n",
    "            sum_N_values(df, channel)\n",
    "        \n",
    "        for channel in channels:\n",
    "            average_zsipm_values(df, channel)\n",
    "            average_ysipm_values(df, channel)\n",
    "            average_xsipm_values(df, channel)\n",
    "\n",
    "    def _sequence_result_convert_to_dataframe(a):\n",
    "        # Step 1: Extract non-parity switches and prepare the initial column for them\n",
    "        non_parity_switches = a.non_parity_switches  # Example: ['Delta_NE', 'Enr']\n",
    "        \n",
    "        # Initialize an empty list for rows to build the DataFrame\n",
    "        rows = []\n",
    "        all_columns = non_parity_switches.copy()  # Columns will be added dynamically based on the data\n",
    "        \n",
    "        # Extract the final_result dictionary and labels from object `a`\n",
    "        final_result = a.blinded.final_result\n",
    "        labels = a.labels\n",
    "        # Step 2: Process each quantity and its variance/chi-square\n",
    "        for switch_values in next(iter(final_result.values())).keys():  # Iterate through the second-level keys\n",
    "            row = list(switch_values)  # Start the row with non-parity switch values\n",
    "            \n",
    "            # Dynamically create columns and append data based on actual matrix size\n",
    "            for quantity in ['C', 'phi', 'omega', 'tau', 'A','Ap', 'Am']:\n",
    "                if quantity in final_result:\n",
    "                    matrix = final_result[quantity][switch_values]\n",
    "                    variance_matrix = final_result[f'd{quantity}2'][switch_values]\n",
    "                    chi_square_matrix = final_result[f'sipm_red_chi_square_{quantity}'][switch_values]\n",
    "                    \n",
    "                    # Iterate over the matrix and populate values for the DataFrame\n",
    "                    for i in range(matrix.shape[0]):\n",
    "                        for j in range(matrix.shape[2]):\n",
    "                            label = labels[i][j] if i < len(labels) and j < len(labels[i]) else f\"unknown_{i}_{j}\"\n",
    "                            \n",
    "                            # Quantity value\n",
    "                            row.append(matrix[i, 0, j, 0, 0, 0, 0])\n",
    "                            column_name = f'{quantity}_{label}'\n",
    "                            if column_name not in all_columns:\n",
    "                                all_columns.append(column_name)\n",
    "                            \n",
    "                            # Uncertainty and scaled uncertainty\n",
    "                            uncertainty = np.sqrt(variance_matrix[i, 0, j, 0, 0, 0, 0])\n",
    "                            scaled_uncertainty = np.sqrt(variance_matrix[i, 0, j, 0, 0, 0, 0] * chi_square_matrix[i, 0, j, 0, 0, 0, 0])\n",
    "                            \n",
    "                            row.append(uncertainty)\n",
    "                            uncertainty_column_name = f'uncertainty_{quantity}_{label}'\n",
    "                            if uncertainty_column_name not in all_columns:\n",
    "                                all_columns.append(uncertainty_column_name)\n",
    "                            \n",
    "                            row.append(scaled_uncertainty)\n",
    "                            scaled_uncertainty_column_name = f'scaleduncertainty_{quantity}_{label}'\n",
    "                            if scaled_uncertainty_column_name not in all_columns:\n",
    "                                all_columns.append(scaled_uncertainty_column_name)\n",
    "            \n",
    "            # Special case for 'N' (no variance or chi-square)\n",
    "            if 'N' in final_result:\n",
    "                matrix_N = final_result['N'][switch_values]\n",
    "                for i in range(matrix_N.shape[0]):\n",
    "                    for j in range(matrix_N.shape[2]):\n",
    "                        label = labels[i][j] if i < len(labels) and j < len(labels[i]) else f\"unknown_{i}_{j}\"\n",
    "                        \n",
    "                        row.append(matrix_N[i, 0, j, 0, 0, 0, 0])\n",
    "                        column_name = f'N_{label}'\n",
    "                        if column_name not in all_columns:\n",
    "                            all_columns.append(column_name)\n",
    "\n",
    "            # Special case for 'xsipm' (no variance or chi-square)\n",
    "            if 'xsipm_summary' in final_result:\n",
    "                matrix_xsipm = final_result['xsipm_summary'][switch_values]\n",
    "                for i in range(matrix_xsipm.shape[0]):\n",
    "                    for j in range(matrix_xsipm.shape[2]):\n",
    "                        label = labels[i][j] if i < len(labels) and j < len(labels[i]) else f\"unknown_{i}_{j}\"\n",
    "                        \n",
    "                        row.append(matrix_xsipm[i, 0, j, 0, 0, 0, 0])\n",
    "                        column_name = f'xsipm_{label}'\n",
    "                        if column_name not in all_columns:\n",
    "                            all_columns.append(column_name)\n",
    "            \n",
    "            # Special case for 'ysipm' (no variance or chi-square)\n",
    "            if 'ysipm_summary' in final_result:\n",
    "                matrix_ysipm = final_result['ysipm_summary'][switch_values]\n",
    "                for i in range(matrix_ysipm.shape[0]):\n",
    "                    for j in range(matrix_ysipm.shape[2]):\n",
    "                        label = labels[i][j] if i < len(labels) and j < len(labels[i]) else f\"unknown_{i}_{j}\"\n",
    "                        \n",
    "                        row.append(matrix_ysipm[i, 0, j, 0, 0, 0, 0])\n",
    "                        column_name = f'ysipm_{label}'\n",
    "                        if column_name not in all_columns:\n",
    "                            all_columns.append(column_name)\n",
    "            \n",
    "            # Special case for 'zsipm' (no variance or chi-square)\n",
    "            if 'zsipm_summary' in final_result:\n",
    "                matrix_zsipm = final_result['zsipm_summary'][switch_values]\n",
    "                for i in range(matrix_zsipm.shape[0]):\n",
    "                    for j in range(matrix_zsipm.shape[2]):\n",
    "                        label = labels[i][j] if i < len(labels) and j < len(labels[i]) else f\"unknown_{i}_{j}\"\n",
    "                        \n",
    "                        row.append(matrix_zsipm[i, 0, j, 0, 0, 0, 0])\n",
    "                        column_name = f'zsipm_{label}'\n",
    "                        if column_name not in all_columns:\n",
    "                            all_columns.append(column_name)\n",
    "                            \n",
    "            \n",
    "            # Append the row to the list of rows\n",
    "            rows.append(row)\n",
    "        \n",
    "        # Step 3: Convert rows into a DataFrame with dynamically created columns\n",
    "        df = pd.DataFrame(rows, columns=all_columns)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _sequence_result_convert_to_sipm_dataframe(a):\n",
    "        \"\"\"\n",
    "        Build a wide DataFrame from the nested '..._summary' dictionaries in \n",
    "        a.sequenceresult.blinded.result_summary, with the logic described.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- 1) Extract all _summary keys from result_summary ---\n",
    "        result_summary = a.sequenceresult.blinded.result_summary\n",
    "        summary_keys = [k for k in result_summary.keys() if k.endswith(\"_summary\")]\n",
    "\n",
    "        # --- 2) Parse the keys into base quantities and their corresponding 'dX2' keys ---\n",
    "        #     e.g. \"C_summary\" -> base=\"C\"; \"dC2_summary\" -> base=\"C\"\n",
    "        #     We store them in a dict: quantities[base] = dict(value_key=\"C_summary\", error_key=\"dC2_summary\")\n",
    "        \n",
    "        def parse_base(key):\n",
    "            \"\"\"Returns (base_str, is_d2) from a key like 'C_summary', 'dC2_summary', etc.\"\"\"\n",
    "            if key.startswith(\"d\") and key.endswith(\"2_summary\"):\n",
    "                # example: dC2_summary\n",
    "                base_str = key[1:-len(\"2_summary\")]  # remove leading 'd' and trailing '2_summary'\n",
    "                return base_str, True\n",
    "            else:\n",
    "                # example: C_summary or phi_summary, etc.\n",
    "                base_str = key[:-len(\"_summary\")]    # remove trailing '_summary'\n",
    "                return base_str, False\n",
    "\n",
    "        quantities = {}\n",
    "        for key in summary_keys:\n",
    "            base, is_d2 = parse_base(key)\n",
    "            if base not in quantities:\n",
    "                quantities[base] = {\"value_key\": None, \"error_key\": None}\n",
    "            if is_d2:\n",
    "                quantities[base][\"error_key\"] = key\n",
    "            else:\n",
    "                quantities[base][\"value_key\"] = key\n",
    "\n",
    "        # --- 3) Collect all possible tuple-keys across all relevant dictionaries ---\n",
    "        all_tuple_keys = set()\n",
    "        for base, dct in quantities.items():\n",
    "            vkey = dct[\"value_key\"]\n",
    "            if vkey in result_summary:\n",
    "                all_tuple_keys.update(result_summary[vkey].keys())\n",
    "            ekey = dct[\"error_key\"]\n",
    "            if ekey in result_summary and ekey is not None:\n",
    "                all_tuple_keys.update(result_summary[ekey].keys())\n",
    "        all_tuple_keys = sorted(all_tuple_keys, key=lambda x: x if isinstance(x, tuple) else (x,))\n",
    "\n",
    "        # For convenience, get an easy reference to the channel strings:\n",
    "        # combine_switches(...) returns (some_matrix, channel_matrix),\n",
    "        # so we want channel_matrix = combine_switches(...)[1]\n",
    "        # Then channel_matrix[a_idx][b_idx] is the channel name.\n",
    "        superblock_labels = a.superblock_parity_labels\n",
    "        block_labels = a.block_parity_labels\n",
    "        _, channel_matrix = combine_switches(superblock_labels, block_labels)\n",
    "\n",
    "        # We'll build data in a \"dict of columns\" style. \n",
    "        # First: columns for each of a.non_parity_switches\n",
    "        columns_data = {}\n",
    "        non_parity_switches = a.non_parity_switches\n",
    "\n",
    "        # Initialize columns for the non-parity-switches\n",
    "        for switch_name in non_parity_switches:\n",
    "            columns_data[switch_name] = []\n",
    "\n",
    "        # Because we'll loop through each row in 'all_tuple_keys', \n",
    "        # we also need to gather the derived columns for each row.\n",
    "        \n",
    "        # A small helper to do a 1/^2 weighted average or sum (if no errors)\n",
    "        def weighted_or_summed_average(values, variances):\n",
    "            \"\"\"\n",
    "            If variances is not None, compute the 1/^2 weighted average.\n",
    "            Otherwise, sum the values (and return None for the 'uncertainty').\n",
    "            \n",
    "            Returns (aggregated_value, aggregated_uncertainty)\n",
    "            If 'variances' is None, aggregated_uncertainty is None.\n",
    "            \"\"\"\n",
    "            values = np.array(values, dtype=float)\n",
    "            if variances is None:\n",
    "                # Sum them\n",
    "                return values.sum(), None\n",
    "            else:\n",
    "                # Weighted average\n",
    "                var_arr = np.array(variances, dtype=float)\n",
    "                inv_var = 1.0 / var_arr\n",
    "                wavg = np.sum(values * inv_var) / np.sum(inv_var)\n",
    "                wvar = 1.0 / np.sum(inv_var)\n",
    "                return wavg, np.sqrt(wvar)\n",
    "\n",
    "        # Next, we want to figure out the shape (A, 1, B, 1, 1, C, 1) for each quantity\n",
    "        # to know how many a_idx, b_idx, c_idx we have.  We'll do that by peeking at\n",
    "        # one typical row (i.e. the first tuple key) if it exists.  Different base quantities\n",
    "        # may have different shapes, so we handle that inside each base quantity loop.\n",
    "\n",
    "        # We'll accumulate rows in a list of dicts, then build a df at the end.\n",
    "        rows = []\n",
    "\n",
    "        for tuple_key in all_tuple_keys:\n",
    "            # Prepare a dictionary for this row\n",
    "            row_dict = {}\n",
    "\n",
    "            # Fill in the non-parity-switches columns from the tuple_key\n",
    "            # The assumption: len(tuple_key) == len(a.non_parity_switches)\n",
    "            for i, switch_name in enumerate(non_parity_switches):\n",
    "                if isinstance(tuple_key, tuple):\n",
    "                    row_dict[switch_name] = tuple_key[i]\n",
    "                else:\n",
    "                    # If for some reason the dictionary is keyed by a single non-tuple\n",
    "                    row_dict[switch_name] = tuple_key\n",
    "\n",
    "            # Now loop over each recognized base quantity\n",
    "            for base, dct in quantities.items():\n",
    "                vkey = dct[\"value_key\"]\n",
    "                ekey = dct[\"error_key\"]\n",
    "                if vkey is None:\n",
    "                    # if there's no real 'value' dictionary for this base, skip\n",
    "                    continue\n",
    "                if vkey not in result_summary:\n",
    "                    # the dictionary doesn't exist, skip\n",
    "                    continue\n",
    "\n",
    "                # Does the error dict exist for this tuple_key?\n",
    "                has_error = (ekey is not None \n",
    "                            and ekey in result_summary \n",
    "                            and tuple_key in result_summary[ekey])\n",
    "\n",
    "                # Retrieve the array for the value\n",
    "                if tuple_key not in result_summary[vkey]:\n",
    "                    # This tuple_key doesn't exist for this vkey, skip\n",
    "                    continue\n",
    "\n",
    "                value_array = result_summary[vkey][tuple_key]\n",
    "                # shape is presumably (A,1,B,1,1,C,1)\n",
    "                # Extract shape\n",
    "                shape = value_array.shape\n",
    "                # Typically: A = shape[0], B = shape[2], C = shape[5]\n",
    "                # We'll check them carefully:\n",
    "                A = shape[0]\n",
    "                B = shape[2]\n",
    "                C = shape[5]\n",
    "\n",
    "                # If we have errors, retrieve the error array:\n",
    "                if has_error:\n",
    "                    error_array = result_summary[ekey][tuple_key]\n",
    "                else:\n",
    "                    error_array = None\n",
    "\n",
    "                # We'll gather all the sipm values in a triple nested loop, \n",
    "                # then compute sipmavg, zplus, zminus, etc. for each (a_idx,b_idx).\n",
    "\n",
    "                for a_idx in range(A):\n",
    "                    for b_idx in range(B):\n",
    "                        channel_name = channel_matrix[a_idx][b_idx]\n",
    "\n",
    "                        # Gather the sipm values (and uncertainties if available)\n",
    "                        sipm_vals = []\n",
    "                        sipm_vars = [] if has_error else None\n",
    "\n",
    "                        for c_idx in range(C):\n",
    "                            val = value_array[a_idx, 0, b_idx, 0, 0, c_idx, 0]\n",
    "                            sipm_vals.append(val)\n",
    "                            if has_error:\n",
    "                                var = error_array[a_idx, 0, b_idx, 0, 0, c_idx, 0]\n",
    "                                sipm_vars.append(var)\n",
    "\n",
    "                        # If C=1 => store only _sipmavg\n",
    "                        # Otherwise store _sipm0,1,... if C>1\n",
    "                        # If C=8 => also store zplus,zminus,xplus,xminus,yplus,yminus\n",
    "\n",
    "                        # 1) Possibly store each sipm individually\n",
    "                        if C > 1:\n",
    "                            for c_idx, val in enumerate(sipm_vals):\n",
    "                                col_name = f\"{base}_{channel_name}_sipm{c_idx}\"\n",
    "                                row_dict[col_name] = val\n",
    "\n",
    "                                # store the uncertainty if we have it\n",
    "                                if has_error:\n",
    "                                    unc_name = f\"uncertainty_{base}_{channel_name}_sipm{c_idx}\"\n",
    "                                    row_dict[unc_name] = np.sqrt(sipm_vars[c_idx])\n",
    "\n",
    "                        # 2) Compute the \"average\" or \"sum\" if no uncertainty\n",
    "                        #    Weighted average if we do have an uncertainty\n",
    "                        avg_val, avg_unc = weighted_or_summed_average(sipm_vals, sipm_vars)\n",
    "                        row_dict[f\"{base}_{channel_name}_sipmavg\"] = avg_val\n",
    "                        if has_error and avg_unc is not None:\n",
    "                            row_dict[f\"uncertainty_{base}_{channel_name}_sipmavg\"] = avg_unc\n",
    "\n",
    "                        # 3) If C=8, compute zplus,zminus,xplus,xminus,yplus,yminus\n",
    "                        #    Indices for zplus = [0,1,2,3], zminus = [4,5,6,7]\n",
    "                        #    etc. Weighted or summed the same way\n",
    "                        if C == 8:\n",
    "                            # Build dictionary of name -> indices\n",
    "                            group_map = {\n",
    "                                'zplus':   [0,1,2,3],\n",
    "                                'zminus':  [4,5,6,7],\n",
    "                                'yplus':   [0,1,4,5],\n",
    "                                'yminus':  [2,3,6,7],\n",
    "                                'xplus':   [0,2,4,6],\n",
    "                                'xminus':  [1,3,5,7],\n",
    "                            }\n",
    "                            for grp_name, idx_list in group_map.items():\n",
    "                                grp_vals = [sipm_vals[i] for i in idx_list]\n",
    "                                grp_vars = ([sipm_vars[i] for i in idx_list] \n",
    "                                            if has_error else None)\n",
    "                                grp_val, grp_unc = weighted_or_summed_average(grp_vals, grp_vars)\n",
    "                                col_name = f\"{base}_{channel_name}_{grp_name}\"\n",
    "                                row_dict[col_name] = grp_val\n",
    "                                if has_error and grp_unc is not None:\n",
    "                                    unc_col_name = f\"uncertainty_{base}_{channel_name}_{grp_name}\"\n",
    "                                    row_dict[unc_col_name] = grp_unc\n",
    "\n",
    "                        # If C=1, we do not create _sipm0,...  We'll rely on the above logic \n",
    "                        # which only created `base_{channel}_sipmavg`.\n",
    "\n",
    "            # Done building columns for this particular row\n",
    "            rows.append(row_dict)\n",
    "\n",
    "        # Convert the list-of-dicts 'rows' into a DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "        # Because different rows might not have exactly the same columns (some base might not exist),\n",
    "        # pandas will fill missing columns with NaN as needed. That is usually fine.\n",
    "\n",
    "        # It's often nice to reorder columns so that the non_parity_switches come first:\n",
    "        # We'll do that by making a list of non-parity-switch columns plus the rest in sorted order.\n",
    "        existing_cols = list(df.columns)\n",
    "        front_cols = [c for c in non_parity_switches if c in existing_cols]\n",
    "        other_cols = [c for c in existing_cols if c not in front_cols]\n",
    "        df = df[front_cols + other_cols]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def saveresults(self):\n",
    "        self.sequenceresult.sequencedf = sequenceCalculator._sequence_result_convert_to_dataframe(self.sequenceresult)\n",
    "        self.sequenceresult.sequencesipmdf = sequenceCalculator._sequence_result_convert_to_sipm_dataframe(self)\n",
    "        sequenceCalculator._block_df_combine_sipm(self.superblock_parity_switches, self.non_parity_switches, self.sequenceresult.blockdf)\n",
    "        with open(os.path.join(self.sequence_result_folder_path, f'sequenceresult_{self.sequenceresult.sequence_name}.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.sequenceresult, f)\n",
    "        self.sequenceresult.sequencesipmdf.to_csv(os.path.join(self.sequence_result_folder_path, f'sequencesipmdf_{self.sequenceresult.sequence_name}.csv'), index=False)\n",
    "        self.sequenceresult.blockdf.to_csv(os.path.join(self.sequence_result_folder_path, f'sequenceblocks_{self.sequenceresult.sequence_name}.csv'), index=False)\n",
    "        self.sequenceresult.sequencedf.to_csv(os.path.join(self.sequence_result_folder_path, f'sequencedf_{self.sequenceresult.sequence_name}.csv'), index=False)\n",
    "\n",
    "        self.sequenceresult.blockdf.to_csv(os.path.join(r\"C:\\ACME_analysis\\multiple_results\\sequenceblock_results\", self.sequenceresult.sequence_string[1:] + \".csv\"), index=False)\n",
    "        self.sequenceresult.sequencedf.to_csv(os.path.join(r\"C:\\ACME_analysis\\multiple_results\\sequencedf_result\", self.sequenceresult.sequence_string[1:] + \".csv\"), index=False)\n",
    "        self.sequenceresult.sequencesipmdf.to_csv(os.path.join(r\"C:\\ACME_analysis\\multiple_results\\sequencesipm_results\", self.sequenceresult.sequence_string[1:] + \".csv\"), index=False)\n",
    "\n",
    "    def output_standard_format(self):\n",
    "        stand_dict = {'blind 5':{}}\n",
    "        r_dict= stand_dict['blind 5']\n",
    "        if not os.path.exists(r\"C:\\standard_format\"):\n",
    "            os.makedirs(r\"C:\\standard_format\")\n",
    "        for i in range(len(self.labels)):\n",
    "            for j in range(len(self.labels[0])):\n",
    "                for (newname, quantity, variance) in [(\"Contrast\", \"C\", \"dC2\"), (\"Phase_blinded\", \"phi\", \"dphi2\"), (\"Omega_blinded\", \"omega\", \"domega2\")]:\n",
    "                    if newname not in r_dict:\n",
    "                        r_dict[newname] = {}\n",
    "                    if self.labels[i][j] not in r_dict[newname]:\n",
    "                        r_dict[newname][self.labels[i][j]] = {}\n",
    "                    r_dict[newname][self.labels[i][j]][\"Mean\"] = self.sequenceresult.blinded.final_result[quantity][()][i,0,j,0,0,0,0]\n",
    "                    r_dict[newname][self.labels[i][j]][\"Sigma\"] = np.sqrt(self.sequenceresult.blinded.final_result[variance][()][i,0,j,0,0,0,0])\n",
    "        with open(r\"C:\\standard_format\\standard\" + self.sequenceresult.sequence_string +\".json\", 'w') as f:\n",
    "            json.dump(stand_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
